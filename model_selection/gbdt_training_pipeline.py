#!/usr/bin/env python3
"""
GBDT Training and Evaluation Pipeline

This module implements the complete two-stage training pipeline described in the research:
- Stage 1: Hyperparameter tuning with learning rate 0.05, max iterations 5000, early stopping 200
- Stage 2: Model evaluation with learning rate 0.015, max iterations 20000, early stopping 500
- 5-fold cross-validation for robust performance evaluation
- Ensemble averaging of 5 classifiers
- Efficiency metric evaluation on test data
"""
GBDT Training and Evaluation Pipeline

This module implements the complete two-stage training pipeline described in the research:
- Stage 1: Hyperparameter tuning with learning rate 0.05, max iterations 5000, early stopping 200
- Stage 2: Model evaluation with learning rate 0.015, max iterations 20000, early stopping 500
- 5-fold cross-validation for robust performance evaluation
- Ensemble averaging of 5 classifiers
- Efficiency metric evaluation on test data
"""

import os
import time
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
from pathlib import Path

# Import GBDT libraries
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, log_loss, classification_report

# Import our utilities
from data_loader import QCDDataLoader, load_qcd_data_simple
from hyperparameter_optimization import GBDTHyperparameterOptimizer

class GBDTTrainingPipeline:
    """
    Complete training pipeline for GBDT comparison following the research methodology.
    """
    
    def __init__(self, random_state: int = 42):
        """
        Initialize the training pipeline.
        
        Args:
            random_state: Random seed for reproducibility
        """
        self.random_state = random_state
        np.random.seed(random_state)
        
        # Algorithm configurations
        self.algorithms = {
            'XGBoost': 'xgboost',
            'LightGBM': 'lightgbm', 
            'CatBoost': 'catboost',
            'GradientBoosting': 'gbm'
        }
        
        # Training configurations from the research table
        self.stage1_config = {
            'learning_rate': 0.05,
            'max_iterations': 5000,
            'early_stopping': 200,
            'description': 'Parameter tuning'
        }
        
        self.stage2_config = {
            'learning_rate': 0.015,
            'max_iterations': 20000,
            'early_stopping': 500,
            'description': 'Models evaluation'
        }
        
        # Results storage
        self.stage1_results = {}
        self.stage2_results = {}
        self.ensemble_models = {}
        self.performance_metrics = {}
        
    def _create_model_with_params(self, algorithm: str, params: Dict[str, Any], stage: int) -> Any:
        """
        Create a model instance with specified parameters for a given stage.
        
        Args:
            algorithm: Algorithm identifier
            params: Hyperparameters
            stage: Training stage (1 or 2)
            
        Returns:
            Configured model instance
        """
        config = self.stage1_config if stage == 1 else self.stage2_config
        
        if algorithm == 'xgboost':
            return xgb.XGBClassifier(
                max_depth=params.get('max_depth', 6),
                min_child_weight=params.get('min_child_weight', 1e-5),
                reg_lambda=params.get('lambda', 1.0),
                subsample=params.get('subsample', 1.0),
                learning_rate=config['learning_rate'],
                n_estimators=config['max_iterations'],
                early_stopping_rounds=config['early_stopping'],
                random_state=self.random_state,
                eval_metric='logloss',
                verbosity=0
            )
            
        elif algorithm == 'lightgbm':
            return lgb.LGBMClassifier(
                max_depth=params.get('max_depth', 6),
                min_data_in_leaf=params.get('min_data_in_leaf', 1),
                reg_lambda=params.get('lambda_l2', 1.0),
                subsample=params.get('subsample', 1.0),
                learning_rate=config['learning_rate'],
                n_estimators=config['max_iterations'],
                early_stopping_rounds=config['early_stopping'],
                random_state=self.random_state,
                verbosity=-1
            )
            
        elif algorithm == 'catboost':
            return cb.CatBoostClassifier(
                depth=params.get('depth', 6),
                min_data_in_leaf=params.get('min_data_in_leaf', 1),
                l2_leaf_reg=params.get('l2_leaf_reg', 1.0),
                subsample=params.get('subsample', 1.0),
                learning_rate=config['learning_rate'],
                iterations=config['max_iterations'],
                early_stopping_rounds=config['early_stopping'],
                random_state=self.random_state,
                verbose=False
            )
            
        elif algorithm == 'gbm':
            return GradientBoostingClassifier(
                max_depth=params.get('max_depth', 6),
                min_samples_leaf=params.get('min_samples_leaf', 1),
                alpha=params.get('ccp_alpha', 0.0),  # For L2 regularization
                subsample=params.get('subsample', 1.0),
                learning_rate=config['learning_rate'],
                n_estimators=config['max_iterations'],
                random_state=self.random_state,
                validation_fraction=0.1,
                n_iter_no_change=config['early_stopping']
            )
        
        else:
            raise ValueError(f"Unsupported algorithm: {algorithm}")
    
    def stage1_parameter_tuning(self, X: np.ndarray, y: np.ndarray, n_trials: int = 100) -> Dict[str, Dict[str, Any]]:
        """
        Stage 1: Parameter tuning using cross-validation as described in the research.
        
        Configuration from research table:
        - Learning rate: 0.05
        - Max iterations: 5000
        - Early stopping: 200
        - Quality estimation: Cross-validation
        
        Args:
            X: Feature matrix (QCD training dataset only)
            y: Target vector
            n_trials: Number of optimization trials per algorithm
            
        Returns:
            Dictionary containing best parameters for each algorithm
        """
        print("STAGE 1: PARAMETER TUNING")
        print("=" * 60)
        print("Configuration:")
        print(f"  Learning rate: {self.stage1_config['learning_rate']}")
        print(f"  Max iterations: {self.stage1_config['max_iterations']}")
        print(f"  Early stopping: {self.stage1_config['early_stopping']}")
        print(f"  Quality estimation: Cross-validation")
        print(f"  Using QCD training dataset only")
        print("\\n")\n        \n        # Initialize hyperparameter optimizer\n        optimizer = GBDTHyperparameterOptimizer(random_state=self.random_state)\n        \n        # Run optimization for all algorithms\n        results = optimizer.optimize_all_algorithms(X, y, n_trials=n_trials, stage=1)\n        \n        # Store results\n        self.stage1_results = results\n        \n        # Extract best parameters\n        best_params = {}\n        for algorithm_name, algorithm_key in self.algorithms.items():\n            if algorithm_key in optimizer.best_params:\n                best_params[algorithm_name] = optimizer.best_params[algorithm_key]\n        \n        return best_params\n    \n    def stage2_model_evaluation(self, X: np.ndarray, y: np.ndarray, \n                               best_params: Dict[str, Dict[str, Any]]) -> Dict[str, List[Any]]:\n        \"\"\"\n        Stage 2: Model evaluation with optimal hyperparameters as described in the research.\n        \n        Configuration from research table:\n        - Learning rate: 0.015\n        - Max iterations: 20000\n        - Early stopping: 500\n        - Quality estimation: Test set\n        \n        Args:\n            X: Feature matrix\n            y: Target vector\n            best_params: Best parameters from stage 1\n            \n        Returns:\n            Dictionary containing trained ensemble models for each algorithm\n        \"\"\"\n        print(\"STAGE 2: MODEL EVALUATION\")\n        print(\"=\" * 60)\n        print(\"Configuration:\")\n        print(f\"  Learning rate: {self.stage2_config['learning_rate']}\")\n        print(f\"  Max iterations: {self.stage2_config['max_iterations']}\")\n        print(f\"  Early stopping: {self.stage2_config['early_stopping']}\")\n        print(f\"  Quality estimation: Test set\")\n        print(f\"  Using optimal hyperparameters from Stage 1\")\n        print(\"\\n\")\n        \n        # 5-fold cross-validation setup\n        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        ensemble_models = {}\n        cv_results = {}\n        \n        for algorithm_name, algorithm_key in self.algorithms.items():\n            print(f\"Training {algorithm_name} with 5-fold cross-validation...\")\n            \n            models = []\n            fold_accuracies = []\n            fold_auc_scores = []\n            fold_log_losses = []\n            \n            # Get best parameters for this algorithm\n            if algorithm_name in best_params:\n                params = best_params[algorithm_name]\n            else:\n                print(f\"Warning: No optimized parameters found for {algorithm_name}, using defaults\")\n                params = {}\n            \n            # Train 5 models using cross-validation\n            for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n                print(f\"  Training fold {fold + 1}/5...\")\n                \n                X_train, X_val = X[train_idx], X[val_idx]\n                y_train, y_val = y[train_idx], y[val_idx]\n                \n                # Create model with stage 2 configuration\n                model = self._create_model_with_params(algorithm_key, params, stage=2)\n                \n                # Train model with early stopping\n                if algorithm_key in ['xgboost', 'lightgbm']:\n                    model.fit(\n                        X_train, y_train,\n                        eval_set=[(X_val, y_val)],\n                        verbose=False\n                    )\n                elif algorithm_key == 'catboost':\n                    model.fit(\n                        X_train, y_train,\n                        eval_set=(X_val, y_val),\n                        verbose=False\n                    )\n                else:  # gbm\n                    model.fit(X_train, y_train)\n                \n                # Evaluate on validation set\n                y_pred = model.predict(X_val)\n                y_pred_proba = model.predict_proba(X_val)[:, 1]\n                \n                accuracy = accuracy_score(y_val, y_pred)\n                auc_score = roc_auc_score(y_val, y_pred_proba)\n                logloss = log_loss(y_val, y_pred_proba)\n                \n                models.append(model)\n                fold_accuracies.append(accuracy)\n                fold_auc_scores.append(auc_score)\n                fold_log_losses.append(logloss)\n                \n                print(f\"    Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, Log Loss: {logloss:.4f}\")\n            \n            # Store ensemble models and CV results\n            ensemble_models[algorithm_name] = models\n            cv_results[algorithm_name] = {\n                'accuracy_mean': np.mean(fold_accuracies),\n                'accuracy_std': np.std(fold_accuracies),\n                'auc_mean': np.mean(fold_auc_scores),\n                'auc_std': np.std(fold_auc_scores),\n                'logloss_mean': np.mean(fold_log_losses),\n                'logloss_std': np.std(fold_log_losses),\n                'fold_results': {\n                    'accuracies': fold_accuracies,\n                    'auc_scores': fold_auc_scores,\n                    'log_losses': fold_log_losses\n                }\n            }\n            \n            print(f\"  {algorithm_name} Average CV Results:\")\n            print(f\"    Accuracy: {cv_results[algorithm_name]['accuracy_mean']:.4f} ± {cv_results[algorithm_name]['accuracy_std']:.4f}\")\n            print(f\"    AUC: {cv_results[algorithm_name]['auc_mean']:.4f} ± {cv_results[algorithm_name]['auc_std']:.4f}\")\n            print(f\"    Log Loss: {cv_results[algorithm_name]['logloss_mean']:.4f} ± {cv_results[algorithm_name]['logloss_std']:.4f}\")\n            print(\"-\" * 40)\n        \n        self.ensemble_models = ensemble_models\n        self.stage2_results = cv_results\n        \n        return ensemble_models\n    \n    def ensemble_predict(self, X: np.ndarray, algorithm: str) -> np.ndarray:\n        \"\"\"\n        Generate ensemble predictions by averaging outputs from 5 classifiers.\n        \n        This implements the research methodology:\n        \"We averaged outputs of the classifiers to obtain a single output vector of class probabilities.\"\n        \n        Args:\n            X: Feature matrix\n            algorithm: Algorithm name\n            \n        Returns:\n            Averaged probability predictions\n        \"\"\"\n        if algorithm not in self.ensemble_models:\n            raise ValueError(f\"Algorithm {algorithm} not trained yet.\")\n        \n        models = self.ensemble_models[algorithm]\n        predictions = []\n        \n        # Get predictions from each of the 5 models\n        for model in models:\n            pred_proba = model.predict_proba(X)[:, 1]\n            predictions.append(pred_proba)\n        \n        # Average predictions from all 5 models\n        ensemble_pred = np.mean(predictions, axis=0)\n        return ensemble_pred\n    \n    def evaluate_efficiency_metrics(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Evaluate all algorithms on test data using efficiency metrics.\n        \n        As stated in the research:\n        \"Consequently, each developed model was evaluated using the efficiency metric on the testing data points.\"\n        \n        Args:\n            X_test: Test feature matrix\n            y_test: Test target vector\n            \n        Returns:\n            Dictionary containing evaluation results for each algorithm\n        \"\"\"\n        print(\"EFFICIENCY METRIC EVALUATION ON TEST SET\")\n        print(\"=\" * 60)\n        \n        results = {}\n        ensemble_predictions = {}\n        \n        for algorithm_name in self.algorithms.keys():\n            if algorithm_name not in self.ensemble_models:\n                print(f\"Warning: {algorithm_name} not trained, skipping evaluation\")\n                continue\n                \n            print(f\"Evaluating {algorithm_name}...\")\n            \n            # Get ensemble predictions\n            ensemble_pred = self.ensemble_predict(X_test, algorithm_name)\n            ensemble_pred_binary = (ensemble_pred > 0.5).astype(int)\n            \n            # Calculate efficiency metrics\n            accuracy = accuracy_score(y_test, ensemble_pred_binary)\n            auc_score = roc_auc_score(y_test, ensemble_pred)\n            logloss = log_loss(y_test, ensemble_pred)\n            \n            # Additional metrics\n            fpr, tpr, _ = roc_curve(y_test, ensemble_pred)\n            \n            results[algorithm_name] = {\n                'accuracy': accuracy,\n                'auc': auc_score,\n                'log_loss': logloss,\n                'fpr': fpr,\n                'tpr': tpr,\n                'ensemble_predictions': ensemble_pred\n            }\n            \n            ensemble_predictions[algorithm_name] = ensemble_pred\n            \n            print(f\"  Accuracy: {accuracy:.4f}\")\n            print(f\"  AUC: {auc_score:.4f}\")\n            print(f\"  Log Loss: {logloss:.4f}\")\n            print(\"-\" * 30)\n        \n        self.performance_metrics = results\n        return results\n    \n    def run_complete_pipeline(self, root_file_path: str = \"data/processed/azura/azura-NewQCD.root\",\n                             test_size: float = 0.2, n_trials: int = 50) -> Dict[str, Any]:\n        \"\"\"\n        Run the complete two-stage GBDT comparison pipeline.\n        \n        Args:\n            root_file_path: Path to the ROOT data file\n            test_size: Proportion of data for testing\n            n_trials: Number of optimization trials per algorithm\n            \n        Returns:\n            Complete results dictionary\n        \"\"\"\n        print(\"GBDT COMPARISON PIPELINE - TWO STAGE APPROACH\")\n        print(\"=\" * 80)\n        print(f\"Dataset: {root_file_path}\")\n        print(f\"Test size: {test_size * 100}%\")\n        print(f\"Optimization trials per algorithm: {n_trials}\")\n        print(f\"Random state: {self.random_state}\")\n        print(\"\\n\")\n        \n        start_time = time.time()\n        \n        # Load data using the research methodology\n        print(\"Loading QCD data...\")\n        data_loader = QCDDataLoader(random_state=self.random_state)\n        \n        try:\n            X, y, metadata = data_loader.load_qcd_data(root_file_path)\n        except FileNotFoundError:\n            print(f\"ROOT file not found: {root_file_path}\")\n            print(\"Using simulated data for demonstration...\")\n            \n            # Create simulated data for demonstration\n            n_samples = 10000\n            n_features = 14  # Number of features from QCD dataset\n            X = np.random.randn(n_samples, n_features)\n            y = np.random.binomial(1, 0.3, n_samples)  # 30% positive class\n            metadata = {'feature_names': [f'feature_{i}' for i in range(n_features)]}\n        \n        print(f\"Data shape: {X.shape}\")\n        print(f\"Class distribution: {np.bincount(y)}\")\n        \n        # Split data into train and test sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n        )\n        \n        print(f\"Training set: {X_train.shape[0]} samples\")\n        print(f\"Test set: {X_test.shape[0]} samples\")\n        print(\"\\n\")\n        \n        # Stage 1: Hyperparameter tuning (using only QCD training dataset)\n        best_params = self.stage1_parameter_tuning(X_train, y_train, n_trials)\n        \n        # Stage 2: Model evaluation with optimal parameters\n        ensemble_models = self.stage2_model_evaluation(X_train, y_train, best_params)\n        \n        # Evaluate on test set\n        test_results = self.evaluate_efficiency_metrics(X_test, y_test)\n        \n        # Calculate total pipeline time\n        total_time = time.time() - start_time\n        \n        # Compile complete results\n        pipeline_results = {\n            'data_info': {\n                'train_samples': X_train.shape[0],\n                'test_samples': X_test.shape[0],\n                'features': X_train.shape[1],\n                'feature_names': metadata.get('feature_names', None)\n            },\n            'stage1_results': self.stage1_results,\n            'stage2_results': self.stage2_results,\n            'best_parameters': best_params,\n            'test_evaluation': test_results,\n            'pipeline_info': {\n                'total_time_seconds': total_time,\n                'n_trials_per_algorithm': n_trials,\n                'random_state': self.random_state\n            }\n        }\n        \n        print(f\"\\nPipeline completed in {total_time:.2f} seconds\")\n        \n        return pipeline_results\n    \n    def print_final_comparison(self, results: Dict[str, Any]) -> None:\n        \"\"\"\n        Print a comprehensive comparison of all GBDT algorithms.\n        \n        Args:\n            results: Complete pipeline results\n        \"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"FINAL GBDT ALGORITHM COMPARISON\")\n        print(\"=\" * 80)\n        \n        # Data information\n        data_info = results['data_info']\n        print(f\"\\nDataset Information:\")\n        print(f\"  Training samples: {data_info['train_samples']:,}\")\n        print(f\"  Test samples: {data_info['test_samples']:,}\")\n        print(f\"  Features: {data_info['features']}\")\n        \n        # Stage 1 results\n        print(f\"\\nStage 1 - Hyperparameter Tuning Results:\")\n        print(f\"  Configuration: LR={self.stage1_config['learning_rate']}, Max iter={self.stage1_config['max_iterations']}, Early stop={self.stage1_config['early_stopping']}\")\n        \n        # Stage 2 results\n        print(f\"\\nStage 2 - Cross-Validation Results:\")\n        print(f\"  Configuration: LR={self.stage2_config['learning_rate']}, Max iter={self.stage2_config['max_iterations']}, Early stop={self.stage2_config['early_stopping']}\")\n        \n        for algorithm in self.algorithms.keys():\n            if algorithm in results['stage2_results']:\n                cv_res = results['stage2_results'][algorithm]\n                print(f\"  {algorithm}:\")\n                print(f\"    CV Accuracy: {cv_res['accuracy_mean']:.4f} ± {cv_res['accuracy_std']:.4f}\")\n                print(f\"    CV AUC: {cv_res['auc_mean']:.4f} ± {cv_res['auc_std']:.4f}\")\n        \n        # Test evaluation results\n        print(f\"\\nTest Set Evaluation (Efficiency Metrics):\")\n        best_algorithm = None\n        best_auc = 0\n        \n        for algorithm in self.algorithms.keys():\n            if algorithm in results['test_evaluation']:\n                test_res = results['test_evaluation'][algorithm]\n                print(f\"  {algorithm}:\")\n                print(f\"    Test Accuracy: {test_res['accuracy']:.4f}\")\n                print(f\"    Test AUC: {test_res['auc']:.4f}\")\n                print(f\"    Test Log Loss: {test_res['log_loss']:.4f}\")\n                \n                if test_res['auc'] > best_auc:\n                    best_auc = test_res['auc']\n                    best_algorithm = algorithm\n        \n        print(f\"\\nBest Performing Algorithm: {best_algorithm} (Test AUC: {best_auc:.4f})\")\n        \n        # Best hyperparameters\n        print(f\"\\nOptimal Hyperparameters:\")\n        for algorithm, params in results['best_parameters'].items():\n            print(f\"  {algorithm}:\")\n            for param, value in params.items():\n                print(f\"    {param}: {value}\")\n        \n        # Pipeline information\n        pipeline_info = results['pipeline_info']\n        print(f\"\\nPipeline Information:\")\n        print(f\"  Total execution time: {pipeline_info['total_time_seconds']:.2f} seconds\")\n        print(f\"  Optimization trials per algorithm: {pipeline_info['n_trials_per_algorithm']}\")\n        print(f\"  Random state: {pipeline_info['random_state']}\")\n    \n    def save_complete_results(self, results: Dict[str, Any], save_dir: str = \"results\") -> None:\n        \"\"\"\n        Save all results including models, parameters, and metrics.\n        \n        Args:\n            results: Complete pipeline results\n            save_dir: Directory to save results\n        \"\"\"\n        # Create save directory\n        save_path = Path(save_dir)\n        save_path.mkdir(parents=True, exist_ok=True)\n        \n        # Save trained models\n        models_path = save_path / \"ensemble_models.pkl\"\n        with open(models_path, 'wb') as f:\n            pickle.dump(self.ensemble_models, f)\n        \n        # Save complete results\n        results_path = save_path / \"complete_results.pkl\"\n        with open(results_path, 'wb') as f:\n            pickle.dump(results, f)\n        \n        # Save best parameters as JSON for easy reading\n        import json\n        params_path = save_path / \"best_parameters.json\"\n        with open(params_path, 'w') as f:\n            json.dump(results['best_parameters'], f, indent=2)\n        \n        print(f\"\\nResults saved to {save_dir}:\")\n        print(f\"  Models: {models_path}\")\n        print(f\"  Complete results: {results_path}\")\n        print(f\"  Best parameters: {params_path}\")\n\n\ndef run_gbdt_comparison_pipeline(root_file_path: str = \"data/processed/azura/azura-NewQCD.root\",\n                                n_trials: int = 50, test_size: float = 0.2, \n                                save_results: bool = True) -> Dict[str, Any]:\n    \"\"\"\n    Run the complete GBDT comparison pipeline following the research methodology.\n    \n    Args:\n        root_file_path: Path to the ROOT data file\n        n_trials: Number of optimization trials per algorithm\n        test_size: Proportion of data for testing\n        save_results: Whether to save results to files\n        \n    Returns:\n        Complete pipeline results\n    \"\"\"\n    # Initialize pipeline\n    pipeline = GBDTTrainingPipeline(random_state=42)\n    \n    # Run complete pipeline\n    results = pipeline.run_complete_pipeline(\n        root_file_path=root_file_path,\n        test_size=test_size,\n        n_trials=n_trials\n    )\n    \n    # Print final comparison\n    pipeline.print_final_comparison(results)\n    \n    # Save results if requested\n    if save_results:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        save_dir = f\"results/gbdt_comparison_{timestamp}\"\n        pipeline.save_complete_results(results, save_dir)\n    \n    return results\n\n\nif __name__ == \"__main__\":\n    print(\"GBDT Training Pipeline\")\n    print(\"=\" * 40)\n    print(\"\\nThis script implements the complete two-stage GBDT comparison pipeline.\")\n    print(\"\\nTo run the pipeline, use:\")\n    print(\"python gbdt_training_pipeline.py\")\n    print(\"\\nOr import and use:\")\n    print(\"from gbdt_training_pipeline import run_gbdt_comparison_pipeline\")\n    print(\"results = run_gbdt_comparison_pipeline()\")\n    \n    # Uncomment the following line to run the pipeline\n    # results = run_gbdt_comparison_pipeline(n_trials=20)  # Reduced trials for testing\n"}}
</function_calls>

<function_results>{"file_content_after_create":{"path":"/Users/khatran/Documents/CERN/cern-source/project/CMS-trigger-efficiency-briar/gbdt_training_pipeline.py","content":"\"\"\"\nGBDT Training and Evaluation Pipeline\n\nThis module implements the complete two-stage training pipeline described in the research:\n- Stage 1: Hyperparameter tuning with learning rate 0.05, max iterations 5000, early stopping 200\n- Stage 2: Model evaluation with learning rate 0.015, max iterations 20000, early stopping 500\n- 5-fold cross-validation for robust performance evaluation\n- Ensemble averaging of 5 classifiers\n- Efficiency metric evaluation on test data\n\"\"\"\n\nimport os\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Import GBDT libraries\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, log_loss, classification_report\n\n# Import our utilities\nfrom data_loader import QCDDataLoader, load_qcd_data_simple\nfrom hyperparameter_optimization import GBDTHyperparameterOptimizer\n\nclass GBDTTrainingPipeline:\n    \"\"\"\n    Complete training pipeline for GBDT comparison following the research methodology.\n    \"\"\"\n    \n    def __init__(self, random_state: int = 42):\n        \"\"\"\n        Initialize the training pipeline.\n        \n        Args:\n            random_state: Random seed for reproducibility\n        \"\"\"\n        self.random_state = random_state\n        np.random.seed(random_state)\n        \n        # Algorithm configurations\n        self.algorithms = {\n            'XGBoost': 'xgboost',\n            'LightGBM': 'lightgbm', \n            'CatBoost': 'catboost',\n            'GradientBoosting': 'gbm'\n        }\n        \n        # Training configurations from the research table\n        self.stage1_config = {\n            'learning_rate': 0.05,\n            'max_iterations': 5000,\n            'early_stopping': 200,\n            'description': 'Parameter tuning'\n        }\n        \n        self.stage2_config = {\n            'learning_rate': 0.015,\n            'max_iterations': 20000,\n            'early_stopping': 500,\n            'description': 'Models evaluation'\n        }\n        \n        # Results storage\n        self.stage1_results = {}\n        self.stage2_results = {}\n        self.ensemble_models = {}\n        self.performance_metrics = {}\n        \n    def _create_model_with_params(self, algorithm: str, params: Dict[str, Any], stage: int) -> Any:\n        \"\"\"\n        Create a model instance with specified parameters for a given stage.\n        \n        Args:\n            algorithm: Algorithm identifier\n            params: Hyperparameters\n            stage: Training stage (1 or 2)\n            \n        Returns:\n            Configured model instance\n        \"\"\"\n        config = self.stage1_config if stage == 1 else self.stage2_config\n        \n        if algorithm == 'xgboost':\n            return xgb.XGBClassifier(\n                max_depth=params.get('max_depth', 6),\n                min_child_weight=params.get('min_child_weight', 1e-5),\n                reg_lambda=params.get('lambda', 1.0),\n                subsample=params.get('subsample', 1.0),\n                learning_rate=config['learning_rate'],\n                n_estimators=config['max_iterations'],\n                early_stopping_rounds=config['early_stopping'],\n                random_state=self.random_state,\n                eval_metric='logloss',\n                verbosity=0\n            )\n            \n        elif algorithm == 'lightgbm':\n            return lgb.LGBMClassifier(\n                max_depth=params.get('max_depth', 6),\n                min_data_in_leaf=params.get('min_data_in_leaf', 1),\n                reg_lambda=params.get('lambda_l2', 1.0),\n                subsample=params.get('subsample', 1.0),\n                learning_rate=config['learning_rate'],\n                n_estimators=config['max_iterations'],\n                early_stopping_rounds=config['early_stopping'],\n                random_state=self.random_state,\n                verbosity=-1\n            )\n            \n        elif algorithm == 'catboost':\n            return cb.CatBoostClassifier(\n                depth=params.get('depth', 6),\n                min_data_in_leaf=params.get('min_data_in_leaf', 1),\n                l2_leaf_reg=params.get('l2_leaf_reg', 1.0),\n                subsample=params.get('subsample', 1.0),\n                learning_rate=config['learning_rate'],\n                iterations=config['max_iterations'],\n                early_stopping_rounds=config['early_stopping'],\n                random_state=self.random_state,\n                verbose=False\n            )\n            \n        elif algorithm == 'gbm':\n            return GradientBoostingClassifier(\n                max_depth=params.get('max_depth', 6),\n                min_samples_leaf=params.get('min_samples_leaf', 1),\n                alpha=params.get('ccp_alpha', 0.0),  # For L2 regularization\n                subsample=params.get('subsample', 1.0),\n                learning_rate=config['learning_rate'],\n                n_estimators=config['max_iterations'],\n                random_state=self.random_state,\n                validation_fraction=0.1,\n                n_iter_no_change=config['early_stopping']\n            )\n        \n        else:\n            raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n    \n    def stage1_parameter_tuning(self, X: np.ndarray, y: np.ndarray, n_trials: int = 100) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Stage 1: Parameter tuning using cross-validation as described in the research.\n        \n        Configuration from research table:\n        - Learning rate: 0.05\n        - Max iterations: 5000\n        - Early stopping: 200\n        - Quality estimation: Cross-validation\n        \n        Args:\n            X: Feature matrix (QCD training dataset only)\n            y: Target vector\n            n_trials: Number of optimization trials per algorithm\n            \n        Returns:\n            Dictionary containing best parameters for each algorithm\n        \"\"\"\n        print(\"STAGE 1: PARAMETER TUNING\")\n        print(\"=\" * 60)\n        print(\"Configuration:\")\n        print(f\"  Learning rate: {self.stage1_config['learning_rate']}\")\n        print(f\"  Max iterations: {self.stage1_config['max_iterations']}\")\n        print(f\"  Early stopping: {self.stage1_config['early_stopping']}\")\n        print(f\"  Quality estimation: Cross-validation\")\n        print(f\"  Using QCD training dataset only\")\n        print(\"\\\\n\")\\n        \\n        # Initialize hyperparameter optimizer\\n        optimizer = GBDTHyperparameterOptimizer(random_state=self.random_state)\\n        \\n        # Run optimization for all algorithms\\n        results = optimizer.optimize_all_algorithms(X, y, n_trials=n_trials, stage=1)\\n        \\n        # Store results\\n        self.stage1_results = results\\n        \\n        # Extract best parameters\\n        best_params = {}\\n        for algorithm_name, algorithm_key in self.algorithms.items():\\n            if algorithm_key in optimizer.best_params:\\n                best_params[algorithm_name] = optimizer.best_params[algorithm_key]\\n        \\n        return best_params\n    \n    def stage2_model_evaluation(self, X: np.ndarray, y: np.ndarray, \n                               best_params: Dict[str, Dict[str, Any]]) -> Dict[str, List[Any]]:\n        \\\"\\\"\\\"\n        Stage 2: Model evaluation with optimal hyperparameters as described in the research.\n        \n        Configuration from research table:\n        - Learning rate: 0.015\n        - Max iterations: 20000\n        - Early stopping: 500\n        - Quality estimation: Test set\n        \n        Args:\n            X: Feature matrix\n            y: Target vector\n            best_params: Best parameters from stage 1\n            \n        Returns:\n            Dictionary containing trained ensemble models for each algorithm\n        \\\"\\\"\\\"\n        print(\\\"STAGE 2: MODEL EVALUATION\\\")\n        print(\\\"=\\\" * 60)\n        print(\\\"Configuration:\\\")\n        print(f\\\"  Learning rate: {self.stage2_config['learning_rate']}\\\")\n        print(f\\\"  Max iterations: {self.stage2_config['max_iterations']}\\\")\n        print(f\\\"  Early stopping: {self.stage2_config['early_stopping']}\\\")\n        print(f\\\"  Quality estimation: Test set\\\")\n        print(f\\\"  Using optimal hyperparameters from Stage 1\\\")\n        print(\\\"\\\\n\\\")\n        \n        # 5-fold cross-validation setup\n        kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)\n        ensemble_models = {}\n        cv_results = {}\n        \n        for algorithm_name, algorithm_key in self.algorithms.items():\n            print(f\\\"Training {algorithm_name} with 5-fold cross-validation...\\\")\n            \n            models = []\n            fold_accuracies = []\n            fold_auc_scores = []\n            fold_log_losses = []\n            \n            # Get best parameters for this algorithm\n            if algorithm_name in best_params:\n                params = best_params[algorithm_name]\n            else:\n                print(f\\\"Warning: No optimized parameters found for {algorithm_name}, using defaults\\\")\n                params = {}\n            \n            # Train 5 models using cross-validation\n            for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n                print(f\\\"  Training fold {fold + 1}/5...\\\")\n                \n                X_train, X_val = X[train_idx], X[val_idx]\n                y_train, y_val = y[train_idx], y[val_idx]\n                \n                # Create model with stage 2 configuration\n                model = self._create_model_with_params(algorithm_key, params, stage=2)\n                \n                # Train model with early stopping\n                if algorithm_key in ['xgboost', 'lightgbm']:\n                    model.fit(\n                        X_train, y_train,\n                        eval_set=[(X_val, y_val)],\n                        verbose=False\n                    )\n                elif algorithm_key == 'catboost':\n                    model.fit(\n                        X_train, y_train,\n                        eval_set=(X_val, y_val),\n                        verbose=False\n                    )\n                else:  # gbm\n                    model.fit(X_train, y_train)\n                \n                # Evaluate on validation set\n                y_pred = model.predict(X_val)\n                y_pred_proba = model.predict_proba(X_val)[:, 1]\n                \n                accuracy = accuracy_score(y_val, y_pred)\n                auc_score = roc_auc_score(y_val, y_pred_proba)\n                logloss = log_loss(y_val, y_pred_proba)\n                \n                models.append(model)\n                fold_accuracies.append(accuracy)\n                fold_auc_scores.append(auc_score)\n                fold_log_losses.append(logloss)\n                \n                print(f\\\"    Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, Log Loss: {logloss:.4f}\\\")\n            \n            # Store ensemble models and CV results\n            ensemble_models[algorithm_name] = models\n            cv_results[algorithm_name] = {\n                'accuracy_mean': np.mean(fold_accuracies),\n                'accuracy_std': np.std(fold_accuracies),\n                'auc_mean': np.mean(fold_auc_scores),\n                'auc_std': np.std(fold_auc_scores),\n                'logloss_mean': np.mean(fold_log_losses),\n                'logloss_std': np.std(fold_log_losses),\n                'fold_results': {\n                    'accuracies': fold_accuracies,\n                    'auc_scores': fold_auc_scores,\n                    'log_losses': fold_log_losses\n                }\n            }\n            \n            print(f\\\"  {algorithm_name} Average CV Results:\\\")\n            print(f\\\"    Accuracy: {cv_results[algorithm_name]['accuracy_mean']:.4f} ± {cv_results[algorithm_name]['accuracy_std']:.4f}\\\")\n            print(f\\\"    AUC: {cv_results[algorithm_name]['auc_mean']:.4f} ± {cv_results[algorithm_name]['auc_std']:.4f}\\\")\n            print(f\\\"    Log Loss: {cv_results[algorithm_name]['logloss_mean']:.4f} ± {cv_results[algorithm_name]['logloss_std']:.4f}\\\")\n            print(\\\"-\\\" * 40)\n        \n        self.ensemble_models = ensemble_models\n        self.stage2_results = cv_results\n        \n        return ensemble_models\n    \n    def ensemble_predict(self, X: np.ndarray, algorithm: str) -> np.ndarray:\n        \\\"\\\"\\\"\n        Generate ensemble predictions by averaging outputs from 5 classifiers.\n        \n        This implements the research methodology:\n        \\\"We averaged outputs of the classifiers to obtain a single output vector of class probabilities.\\\"\n        \n        Args:\n            X: Feature matrix\n            algorithm: Algorithm name\n            \n        Returns:\n            Averaged probability predictions\n        \\\"\\\"\\\"\n        if algorithm not in self.ensemble_models:\n            raise ValueError(f\\\"Algorithm {algorithm} not trained yet.\\\")\n        \n        models = self.ensemble_models[algorithm]\n        predictions = []\n        \n        # Get predictions from each of the 5 models\n        for model in models:\n            pred_proba = model.predict_proba(X)[:, 1]\n            predictions.append(pred_proba)\n        \n        # Average predictions from all 5 models\n        ensemble_pred = np.mean(predictions, axis=0)\n        return ensemble_pred\n    \n    def evaluate_efficiency_metrics(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict[str, float]]:\n        \\\"\\\"\\\"\n        Evaluate all algorithms on test data using efficiency metrics.\n        \n        As stated in the research:\n        \\\"Consequently, each developed model was evaluated using the efficiency metric on the testing data points.\\\"\n        \n        Args:\n            X_test: Test feature matrix\n            y_test: Test target vector\n            \n        Returns:\n            Dictionary containing evaluation results for each algorithm\n        \\\"\\\"\\\"\n        print(\\\"EFFICIENCY METRIC EVALUATION ON TEST SET\\\")\n        print(\\\"=\\\" * 60)\n        \n        results = {}\n        ensemble_predictions = {}\n        \n        for algorithm_name in self.algorithms.keys():\n            if algorithm_name not in self.ensemble_models:\n                print(f\\\"Warning: {algorithm_name} not trained, skipping evaluation\\\")\n                continue\n                \n            print(f\\\"Evaluating {algorithm_name}...\\\")\n            \n            # Get ensemble predictions\n            ensemble_pred = self.ensemble_predict(X_test, algorithm_name)\n            ensemble_pred_binary = (ensemble_pred > 0.5).astype(int)\n            \n            # Calculate efficiency metrics\n            accuracy = accuracy_score(y_test, ensemble_pred_binary)\n            auc_score = roc_auc_score(y_test, ensemble_pred)\n            logloss = log_loss(y_test, ensemble_pred)\n            \n            # Additional metrics\n            fpr, tpr, _ = roc_curve(y_test, ensemble_pred)\n            \n            results[algorithm_name] = {\n                'accuracy': accuracy,\n                'auc': auc_score,\n                'log_loss': logloss,\n                'fpr': fpr,\n                'tpr': tpr,\n                'ensemble_predictions': ensemble_pred\n            }\n            \n            ensemble_predictions[algorithm_name] = ensemble_pred\n            \n            print(f\\\"  Accuracy: {accuracy:.4f}\\\")\n            print(f\\\"  AUC: {auc_score:.4f}\\\")\n            print(f\\\"  Log Loss: {logloss:.4f}\\\")\n            print(\\\"-\\\" * 30)\n        \n        self.performance_metrics = results\n        return results\n    \n    def run_complete_pipeline(self, root_file_path: str = \\\"data/processed/azura/azura-NewQCD.root\\\",\n                             test_size: float = 0.2, n_trials: int = 50) -> Dict[str, Any]:\n        \\\"\\\"\\\"\n        Run the complete two-stage GBDT comparison pipeline.\n        \n        Args:\n            root_file_path: Path to the ROOT data file\n            test_size: Proportion of data for testing\n            n_trials: Number of optimization trials per algorithm\n            \n        Returns:\n            Complete results dictionary\n        \\\"\\\"\\\"\n        print(\\\"GBDT COMPARISON PIPELINE - TWO STAGE APPROACH\\\")\n        print(\\\"=\\\" * 80)\n        print(f\\\"Dataset: {root_file_path}\\\")\n        print(f\\\"Test size: {test_size * 100}%\\\")\n        print(f\\\"Optimization trials per algorithm: {n_trials}\\\")\n        print(f\\\"Random state: {self.random_state}\\\")\n        print(\\\"\\\\n\\\")\n        \n        start_time = time.time()\n        \n        # Load data using the research methodology\n        print(\\\"Loading QCD data...\\\")\n        data_loader = QCDDataLoader(random_state=self.random_state)\n        \n        try:\n            X, y, metadata = data_loader.load_qcd_data(root_file_path)\n        except FileNotFoundError:\n            print(f\\\"ROOT file not found: {root_file_path}\\\")\n            print(\\\"Using simulated data for demonstration...\\\")\n            \n            # Create simulated data for demonstration\n            n_samples = 10000\n            n_features = 14  # Number of features from QCD dataset\n            X = np.random.randn(n_samples, n_features)\n            y = np.random.binomial(1, 0.3, n_samples)  # 30% positive class\n            metadata = {'feature_names': [f'feature_{i}' for i in range(n_features)]}\n        \n        print(f\\\"Data shape: {X.shape}\\\")\n        print(f\\\"Class distribution: {np.bincount(y)}\\\")\n        \n        # Split data into train and test sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n        )\n        \n        print(f\\\"Training set: {X_train.shape[0]} samples\\\")\n        print(f\\\"Test set: {X_test.shape[0]} samples\\\")\n        print(\\\"\\\\n\\\")\n        \n        # Stage 1: Hyperparameter tuning (using only QCD training dataset)\n        best_params = self.stage1_parameter_tuning(X_train, y_train, n_trials)\n        \n        # Stage 2: Model evaluation with optimal parameters\n        ensemble_models = self.stage2_model_evaluation(X_train, y_train, best_params)\n        \n        # Evaluate on test set\n        test_results = self.evaluate_efficiency_metrics(X_test, y_test)\n        \n        # Calculate total pipeline time\n        total_time = time.time() - start_time\n        \n        # Compile complete results\n        pipeline_results = {\n            'data_info': {\n                'train_samples': X_train.shape[0],\n                'test_samples': X_test.shape[0],\n                'features': X_train.shape[1],\n                'feature_names': metadata.get('feature_names', None)\n            },\n            'stage1_results': self.stage1_results,\n            'stage2_results': self.stage2_results,\n            'best_parameters': best_params,\n            'test_evaluation': test_results,\n            'pipeline_info': {\n                'total_time_seconds': total_time,\n                'n_trials_per_algorithm': n_trials,\n                'random_state': self.random_state\n            }\n        }\n        \n        print(f\\\"\\\\nPipeline completed in {total_time:.2f} seconds\\\")\n        \n        return pipeline_results\n    \n    def print_final_comparison(self, results: Dict[str, Any]) -> None:\n        \\\"\\\"\\\"\n        Print a comprehensive comparison of all GBDT algorithms.\n        \n        Args:\n            results: Complete pipeline results\n        \\\"\\\"\\\"\n        print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\n        print(\\\"FINAL GBDT ALGORITHM COMPARISON\\\")\n        print(\\\"=\\\" * 80)\n        \n        # Data information\n        data_info = results['data_info']\n        print(f\\\"\\\\nDataset Information:\\\")\n        print(f\\\"  Training samples: {data_info['train_samples']:,}\\\")\n        print(f\\\"  Test samples: {data_info['test_samples']:,}\\\")\n        print(f\\\"  Features: {data_info['features']}\\\")\n        \n        # Stage 1 results\n        print(f\\\"\\\\nStage 1 - Hyperparameter Tuning Results:\\\")\n        print(f\\\"  Configuration: LR={self.stage1_config['learning_rate']}, Max iter={self.stage1_config['max_iterations']}, Early stop={self.stage1_config['early_stopping']}\\\")\n        \n        # Stage 2 results\n        print(f\\\"\\\\nStage 2 - Cross-Validation Results:\\\")\n        print(f\\\"  Configuration: LR={self.stage2_config['learning_rate']}, Max iter={self.stage2_config['max_iterations']}, Early stop={self.stage2_config['early_stopping']}\\\")\n        \n        for algorithm in self.algorithms.keys():\n            if algorithm in results['stage2_results']:\n                cv_res = results['stage2_results'][algorithm]\n                print(f\\\"  {algorithm}:\\\")\n                print(f\\\"    CV Accuracy: {cv_res['accuracy_mean']:.4f} ± {cv_res['accuracy_std']:.4f}\\\")\n                print(f\\\"    CV AUC: {cv_res['auc_mean']:.4f} ± {cv_res['auc_std']:.4f}\\\")\n        \n        # Test evaluation results\n        print(f\\\"\\\\nTest Set Evaluation (Efficiency Metrics):\\\")\n        best_algorithm = None\n        best_auc = 0\n        \n        for algorithm in self.algorithms.keys():\n            if algorithm in results['test_evaluation']:\n                test_res = results['test_evaluation'][algorithm]\n                print(f\\\"  {algorithm}:\\\")\n                print(f\\\"    Test Accuracy: {test_res['accuracy']:.4f}\\\")\n                print(f\\\"    Test AUC: {test_res['auc']:.4f}\\\")\n                print(f\\\"    Test Log Loss: {test_res['log_loss']:.4f}\\\")\n                \n                if test_res['auc'] > best_auc:\n                    best_auc = test_res['auc']\n                    best_algorithm = algorithm\n        \n        print(f\\\"\\\\nBest Performing Algorithm: {best_algorithm} (Test AUC: {best_auc:.4f})\\\")\n        \n        # Best hyperparameters\n        print(f\\\"\\\\nOptimal Hyperparameters:\\\")\n        for algorithm, params in results['best_parameters'].items():\n            print(f\\\"  {algorithm}:\\\")\n            for param, value in params.items():\n                print(f\\\"    {param}: {value}\\\")\n        \n        # Pipeline information\n        pipeline_info = results['pipeline_info']\n        print(f\\\"\\\\nPipeline Information:\\\")\n        print(f\\\"  Total execution time: {pipeline_info['total_time_seconds']:.2f} seconds\\\")\n        print(f\\\"  Optimization trials per algorithm: {pipeline_info['n_trials_per_algorithm']}\\\")\n        print(f\\\"  Random state: {pipeline_info['random_state']}\\\")\n    \n    def save_complete_results(self, results: Dict[str, Any], save_dir: str = \\\"results\\\") -> None:\n        \\\"\\\"\\\"\n        Save all results including models, parameters, and metrics.\n        \n        Args:\n            results: Complete pipeline results\n            save_dir: Directory to save results\n        \\\"\\\"\\\"\n        # Create save directory\n        save_path = Path(save_dir)\n        save_path.mkdir(parents=True, exist_ok=True)\n        \n        # Save trained models\n        models_path = save_path / \\\"ensemble_models.pkl\\\"\n        with open(models_path, 'wb') as f:\n            pickle.dump(self.ensemble_models, f)\n        \n        # Save complete results\n        results_path = save_path / \\\"complete_results.pkl\\\"\n        with open(results_path, 'wb') as f:\n            pickle.dump(results, f)\n        \n        # Save best parameters as JSON for easy reading\n        import json\n        params_path = save_path / \\\"best_parameters.json\\\"\n        with open(params_path, 'w') as f:\n            json.dump(results['best_parameters'], f, indent=2)\n        \n        print(f\\\"\\\\nResults saved to {save_dir}:\\\")\n        print(f\\\"  Models: {models_path}\\\")\n        print(f\\\"  Complete results: {results_path}\\\")\n        print(f\\\"  Best parameters: {params_path}\\\")\n\n\ndef run_gbdt_comparison_pipeline(root_file_path: str = \\\"data/processed/azura/azura-NewQCD.root\\\",\n                                n_trials: int = 50, test_size: float = 0.2, \n                                save_results: bool = True) -> Dict[str, Any]:\n    \\\"\\\"\\\"\n    Run the complete GBDT comparison pipeline following the research methodology.\n    \n    Args:\n        root_file_path: Path to the ROOT data file\n        n_trials: Number of optimization trials per algorithm\n        test_size: Proportion of data for testing\n        save_results: Whether to save results to files\n        \n    Returns:\n        Complete pipeline results\n    \\\"\\\"\\\"\n    # Initialize pipeline\n    pipeline = GBDTTrainingPipeline(random_state=42)\n    \n    # Run complete pipeline\n    results = pipeline.run_complete_pipeline(\n        root_file_path=root_file_path,\n        test_size=test_size,\n        n_trials=n_trials\n    )\n    \n    # Print final comparison\n    pipeline.print_final_comparison(results)\n    \n    # Save results if requested\n    if save_results:\n        timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\n        save_dir = f\\\"results/gbdt_comparison_{timestamp}\\\"\n        pipeline.save_complete_results(results, save_dir)\n    \n    return results\n\n\nif __name__ == \\\"__main__\\\":\n    print(\\\"GBDT Training Pipeline\\\")\n    print(\\\"=\\\" * 40)\n    print(\\\"\\\\nThis script implements the complete two-stage GBDT comparison pipeline.\\\")\n    print(\\\"\\\\nTo run the pipeline, use:\\\")\n    print(\\\"python gbdt_training_pipeline.py\\\")\n    print(\\\"\\\\nOr import and use:\\\")\n    print(\\\"from gbdt_training_pipeline import run_gbdt_comparison_pipeline\\\")\n    print(\\\"results = run_gbdt_comparison_pipeline()\\\")\n    \n    # Uncomment the following line to run the pipeline\n    # results = run_gbdt_comparison_pipeline(n_trials=20)  # Reduced trials for testing\n\"\n"}}
</function_calls>

<function_calls>
<invoke name="mark_todo_as_done">
<parameter name="todo_ids">["e9cf811c-b030-49bf-a3c7-37dc8fba97b8"]
